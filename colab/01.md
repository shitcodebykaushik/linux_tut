- Colab (Colaboratory is cloud based ) is hosted jupyter notebook service that require no setup to use and provides free access to computing resource,including GPU's and the TPUs .
- Colab TPUs are known as the Tensor processing Unit .
- TPU are designed for training the neural networks .
- A GPU is a specialized processor originally designed for manipulating computer graphics  and theire parallel structure makes them ideal for algorithms that process large blocks of data commonly found in the AI workloads .
-  A TPU is an application-specific integrated circuit (AISC) designed by Google for neural networks . TPU possess specialized fatures such as the matrix multiply UNIT (MXU) and proprietry  interconnect topology  that make them ideal for acceleration AI taining and interface .
- Trillium is the most advance version of the TPUs as of now .
- We have cloud TPU that are designed to scale cost-effecitrly for a wide range of the AI workloads .
- TPU have on-chip high-bandwidth memory (HBM) that let you use larger models and batch size . TPUs can be connected in groups called PODS that scale up the workloads .
- A CPU is general purpose processor based on the von Neuman architecture, that means a cpu works with the software and memory . The greates benefit of the CPUs is their flexibilty . We can load any kind of the software on the CPU for many different types of the application . A CPU loads values from memory perform a calculation on the values and store the result back in the memory for every calculation . Memory access is slow when compared to the calculation speed and can limit the total througphut of the CPUs . 
- To gain the higher throughput GPUs contains thosands of ALU in a single processor . A modern gpu ususally containts 2500+ alu , the large number of processor means we can execute thousands of multiplication and addition simulataneously . GPU have the same problem like cpu here also for every calculation in the thousands of ALUs, a GPU must access register or shred memory to read the operands and store the intermediate calculation results .
-  Now here comes the Google TPU that can not run the rocket engine or execute the bank transactions but they can handle the massive matrix operations used in neural networks at fast speeds . The primary task for the TPUs is matrix processing which is the combination of multiply and accumulate operations , TPUs contains the thousands of multiply-accumalators that are directly connected to each other to form the large pbysical matrix . This is called systolic array architecture  . TPU host strams data into a an infeed queue , the tpu loads data from the infeed queue ans stores them on HBM memory . When the computation is completed then the TPU loads result into the outfeed queue . The TPU host then reads the results from the outfeed queue and stores them in the host memory .
- Code that run on TPUs must be complied by the accelerator linear algebra (XLA) compiler . XLS is just-in-time compiliar that graph emitted by an ML framework application and complies the linear algebra 